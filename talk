Have you ever been in conversation with someone when you miss the last sentence they said. Maybe they weren't clear or you weren't paying attention. You ask them to repeat themselves but just as they are about to speak your brain pops out a sentence and you think, "Ah, that's what they said."

This happens to me all the time. What it demonstrates is that the brain is continuously deciphering auditory stimulus even seconds after it was received.

What I'm going to show you today is that the act of listening is not purely a mechanical process. Though listening is dependent on physical sound waves, the perception of that sound is dependent on the internal state of the mind of the listener.

My name is Kayhan and this talk is called "Listening for Information".

(Information solves uncertainty)

It seems fairly obvious that one listens with ones ear and brain. We all know that we listen with our ears. For example lets say I play some music and then cover my ears, I have an experiment which shows the role of the ear in capturing sound. But how would you show the role of the brain in interpreting it. Or how can you show that the brain can also be tricked into listening differently.
When put in those words it seems fairly obvious but, it's not always easy to demostrate this. We're going to try and do that now.

So what I'll do is play you an audio clip of a person speaking that has been distorted.

*play*

If anyone caught any words raise your hand and keep it up.

I'll play it once more.

*play*

Any more words? Ok now I'll tell you the real sentence. The sentence was, "Mozart was born on the 27th of January 1756."

Now let's play it once more.

*play*

How many of you heard it? The funny thing is now when I play that audio clip the sentence about Mozart is the only thing you'll be able to hear. In a way you can never go back to hearing the gibberish.

Well that's kind of cool. But let me highlight some interesting things I learnt while testing various distortions. The distortions were created by trial and error and it tooks ome time to find one that was neither too hard nor too easy to understand.

Speech is more than content and context. It is also form. How we speak. This is called prosody and deals with properties of syllables and units of speech.

It was virtually impossible to identify the distorted sentence when a computer did the talking, even when you knew the sentence. This makes one wonder what we listen for when we listen to speech. Content and context and rich but what may be more important is form or prosody. How we speak. For example some aspects of form or prosody in linguistics would be intonation, pause and rythm. I would argue that the nature of the distortion is not as important as the nature of the recording.

Another interesting finding was that there was some variation between listeners. Some listeners heard more words than others in the distorted sentences.

It could be that these listeners don't rely on form cues as much as others so these structures in their brains are weaker. It would be interesting to test voice actors.

Could it be possible to train someone to always understand these distorted sentences? It would almost be like learning a very strange foreign english accent. If so it would be possible to create a simple code or public cryptography.

How many different sentences can be heard. Strangely there is more information in a distorted sentence.

What would happen if I only told you half of a distorted sentence?

A small dark look in his face.
A small dog licking his face.

I'd like to thank Marc Burns for helping me figure out how to distort my audio samples and all the participants who I put through sometimes painful auditory stimulus. If you're interested in anything I talked about come speak to me in the break.



(is concerned with those elements of speech that are not individual phonetic segments (vowels and consonants) but are properties of syllables and larger units of speech.))















